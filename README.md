# ü¶ô Chatbot IA 100 % Local en 15 min

> D√©monstration compl√®te (installation ‚ûú API ‚ûú UI ‚ûú bonus vocal)  
> Public cible : curieux d‚ÄôIA, devs, formateurs, mais aussi non-tech.

---

## üåü Pourquoi ce projet ?

| Probl√®me                                                   | Solution apport√©e                         |
|------------------------------------------------------------|------------------------------------------|
| Faire tourner un LLM sans cl√© API ni cloud                 | **Ollama** sert les mod√®les localement   |
| Tester rapidement plusieurs mod√®les (Llama 3, Gemma‚Ä¶)      | Dropdown de s√©lection en un clic         |
| Partager une d√©mo graphique sans se battre avec le front   | **Gradio** g√©n√®re l‚ÄôUI instantan√©ment    |
| Ajouter la voix sans service externe                       | **Whisper** transcrit hors-ligne         |

---

## ‚ú® Fonctionnalit√©s

- **Serveur Ollama** int√©gr√© : ex√©cute et g√®re les mod√®les open-source.  
- **Chatbot Web** minimal (Gradio) : bulles, th√®me Soft, responsive.  
- **S√©lecteur de mod√®les** : changez de LLM en temps r√©el, sans reload.  
- **Transcription vocale locale** (faster-whisper) : parlez, transcrivez, envoyez.  
- **100 % offline** : aucune donn√©e ne sort de la machine.  

---

## üì∏ Capture d‚Äô√©cran

<p align="center">
 <img src="assets/screenshot_ui.png" width="600" alt="UI Gradio">
</p>

---

## üöÄ Installation rapide

```bash
# 1. Cloner
git clone https://github.com/<ton-user>/ollama-local-chat.git
cd ollama-local-chat

# 2. Installer d√©pendances Python
pip install -r requirements.txt   # gradio, requests, faster-whisper, ffmpeg-python

# 3. Installer Ollama et t√©l√©charger deux mod√®les
winget install Ollama             # ou brew / apt
ollama pull llama3
ollama pull gemma:7b

# 4. Lancer le serveur
ollama serve &                    # service HTTP sur :11434

# 5. Lancer l‚Äôinterface
python ollama_chat_ui.py

# 6. Naviguer ‚ûú http://localhost:7860
